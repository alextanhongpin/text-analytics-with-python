{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet\n",
    "\n",
    "WordNet is a huge database for the english language.\n",
    "\n",
    "## Synsets\n",
    "\n",
    "Synset is a collection or set of data entities that are considered to be semantically similar.\n",
    "\n",
    "## Repo\n",
    "https://github.com/dipanjanS/text-analytics-with-python/tree/master/Old-First-Edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "\n",
    "term = 'fruit'\n",
    "synsets = wn.synsets(term)\n",
    "print('Total Synsets:', len(synsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in synsets:\n",
    "    print('Synset:', synset)\n",
    "    print('Part of Speech:', synset.lexname())\n",
    "    print('Definition:', synset.definition())\n",
    "    print('Lemmas:', synset.lemma_names())\n",
    "    print('Examples:', synset.examples())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entailments\n",
    "\n",
    "The term entailments usually refers to some event or action that logically involves or is associated with some other action or event that has taken place or will take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in ['walk', 'eat', 'digest']:\n",
    "    action_syn = wn.synsets(action, pos='v')[0]\n",
    "    print(action_syn, '-- entails -->', action_syn.entailments())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homonyms and Homographs\n",
    "\n",
    "Homonyms refer to words or terms having the same written form or pronunciation but different meanings. Homonyms are a superset of homographs, which are words with same spelling but may have different pronunciation and meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in wn.synsets('bank'):\n",
    "    print(synset.name(), '-', synset.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonyms and antonyms\n",
    "\n",
    "Synonyms are words having similar meaning and context, and antonyms are words having opposite or contrasting meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'large'\n",
    "synsets = wn.synsets(term)\n",
    "adj_large = synsets[1]\n",
    "adj_large = adj_large.lemmas()[0]\n",
    "adj_large_synonym = adj_large.synset()\n",
    "adj_large_antonym = adj_large.antonyms()[0].synset()\n",
    "\n",
    "# Print synonym and antonym.\n",
    "print('Synonym:', adj_large_synonym.name())\n",
    "print('Definition:', adj_large_synonym.definition())\n",
    "print('Antonym:', adj_large_antonym.name())\n",
    "print('Definition:', adj_large_antonym.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'rich'\n",
    "synsets = wn.synsets(term)[:3]\n",
    "\n",
    "# Print synonym and antonym for different synsets.\n",
    "for synset in synsets:\n",
    "    rich = synset.lemmas()[0]\n",
    "    rich_synonym = rich.synset()\n",
    "    rich_antonym = rich.antonyms()[0].synset()\n",
    "    \n",
    "    print('Synonym:', rich_synonym.name())\n",
    "    print('Definition:', rich_synonym.definition())\n",
    "\n",
    "\n",
    "    print('Antonym:', rich_antonym.name())\n",
    "    print('Definition:', rich_antonym.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyponyms and Hypernyms\n",
    "\n",
    "Hyponym refers to entities or concepts that are a subclass of a higher order concept or entity and have very specific sense or context compared to its superclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'tree'\n",
    "synsets = wn.synsets(term)\n",
    "tree = synsets[0]\n",
    "\n",
    "# Print the entity and its meaning.\n",
    "print('Name:', tree.name())\n",
    "print('Definition:', tree.definition())\n",
    "\n",
    "# Print total hyponyms and some sample hyponyms for 'tree'.\n",
    "hyponyms = tree.hyponyms()\n",
    "print('Total Hyponyms:', len(hyponyms))\n",
    "print('Sample Hyponyms')\n",
    "for hyponym in hyponyms[:10]:\n",
    "    print(hyponym.name(), '-', hyponym.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernyms = tree.hypernyms()\n",
    "print(hypernyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total hierarchy pathways for tree.\n",
    "hypernym_paths = tree.hypernym_paths()\n",
    "print('Total Hypernym paths:', len(hypernym_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the entire hypernym hierarchy.\n",
    "print('Hypernym Hierarchy')\n",
    "print(' -> '.join(synset.name() for synset in hypernym_paths[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holonyms and Meronyms\n",
    "\n",
    "\n",
    "Holonyms are entities that contains a specific entity of our interest. Basically holonyms refers to the relationship between a term or entity that denotes the whole and a term denoting a specific part of the whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "member_holonyms = tree.member_holonyms()\n",
    "print('Total member holonyms:', len(member_holonyms))\n",
    "print('Member holonyms for [tree]:-')\n",
    "for holonym in member_holonyms:\n",
    "    print(holonym.name(), '-', holonym.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part based meronyms for tree.\n",
    "part_meronyms = tree.part_meronyms()\n",
    "print('Total Part Meronyms:', len(part_meronyms))\n",
    "print('Part Meronyms for [tree]:-')\n",
    "for meronym in part_meronyms:\n",
    "    print(meronym.name(), '-', meronym.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substance based meronyms for tree.\n",
    "substance_meronyms = tree.substance_meronyms()\n",
    "print('Total substance meronyms:', len(substance_meronyms))\n",
    "for meronym in substance_meronyms:\n",
    "    print(meronym.name(), '-', meronym.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic relationships and similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = wn.synset('tree.n.01')\n",
    "lion = wn.synset('lion.n.01')\n",
    "tiger = wn.synset('tiger.n.02')\n",
    "cat = wn.synset('cat.n.01')\n",
    "dog = wn.synset('dog.n.01')\n",
    "\n",
    "# Create entities and extract names and definitions.\n",
    "entities = [tree, lion, tiger, cat, dog]\n",
    "entity_names = [entity.name().split('.')[0] for entity in entities]\n",
    "entity_definitions = [entity.definition() for entity in entities]\n",
    "\n",
    "# Print entities and their definitions.\n",
    "for entity, definition in zip(entity_names, entity_definitions):\n",
    "    print(entity, '-', definition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_hypernyms = []\n",
    "for entity in entities:\n",
    "    # Get pairwise lowest common hypernyms.\n",
    "    common_hypernyms.append([entity.lowest_common_hypernyms(compared_entity)[0].name().split('.')[0]\n",
    "                             for compared_entity in entities])\n",
    "    \n",
    "# Build pairwise lower common hypernym matrix.\n",
    "common_hypernym_frame = pd.DataFrame(common_hypernyms,\n",
    "                                     index=entity_names,\n",
    "                                     columns=entity_names)\n",
    "\n",
    "# Print the matrix.\n",
    "common_hypernym_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = []\n",
    "for entity in entities:\n",
    "    # Get pairwise similarities.\n",
    "    similarities.append([round(entity.path_similarity(compared_entity), 2)\n",
    "                         for compared_entity in entities])\n",
    "\n",
    "# Build pairwise similarity matrix.\n",
    "similarity_frame = pd.DataFrame(similarities,\n",
    "                                index=entity_names,\n",
    "                                columns=entity_names)\n",
    "\n",
    "similarity_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word sense disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text and word to disambiguate.\n",
    "samples = [('The fruits on that plant has ripened', 'n'),\n",
    "           ('He finally reaped the fruit of his hard work as he won the race', 'n')]\n",
    "word = 'fruit'\n",
    "\n",
    "# Perform word sense disambiguation.\n",
    "for sentence, pos_tag in samples:\n",
    "    word_syn = lesk(word_tokenize(sentence.lower()), word, pos_tag)\n",
    "    print('Sentence:', sentence)\n",
    "    print('Word synset:', word_syn)\n",
    "    print('Corresponding definition:', word_syn.definition())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text and word to disambiguate.\n",
    "samples = [('Lead is a very soft, malleable metal', 'n'),\n",
    "           ('John is the actor who plays the lead in that movie', 'n'),\n",
    "           ('This road leads to nowhere', 'v')]\n",
    "word = 'lead'\n",
    "\n",
    "# Perform word sense disambiguation.\n",
    "for sentence, pos_tag in samples:\n",
    "    word_syn = lesk(word_tokenize(sentence.lower()), word, pos_tag)\n",
    "    print('Sentence:', sentence)\n",
    "    print('Word synset:', word_syn)\n",
    "    print('Corresponding definition:', word_syn.definition())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition\n",
    "NER, also known as entity chunking/extraction is a popular technique used in information extraction to identify and segment named entities and classify or categorize them under various predefined classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Bayern Munich, or FC Bayern, is a German sports club based in Munich, \n",
    "Bavaria, Germany. It is best known for its professional football team, \n",
    "which plays in the Bundesliga, the top tier of the German football \n",
    "league system, and is the most successful club in German football \n",
    "history, having won a record 26 national titles and 18 national cups. \n",
    "FC Bayern was founded in 1900 by eleven football players led by Franz John. \n",
    "Although Bayern won its first national championship in 1932, the club \n",
    "was not selected for the Bundesliga at its inception in 1963. The club \n",
    "had its period of greatest success in the middle of the 1970s when, \n",
    "under the captaincy of Franz Beckenbauer, it won the European Cup three \n",
    "times in a row (1974-76). Overall, Bayern has reached ten UEFA Champions \n",
    "League finals, most recently winning their fifth title in 2013 as part \n",
    "of a continental treble. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from module.normalization import parse_document\n",
    "import pandas as pd\n",
    "\n",
    "# Tokenize sentences.\n",
    "sentences = parse_document(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) \n",
    "                       for sentence in sentences]\n",
    "\n",
    "# Tag sentences and use nltk's Named Entity Chunker.\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "ne_chunked_sents = [nltk.ne_chunk(tagged) for tagged in tagged_sentences]\n",
    "\n",
    "\n",
    "# Extract all named entities.\n",
    "named_entities = []\n",
    "for ne_tagged_sentence in ne_chunked_sents:\n",
    "    for tagged_tree in ne_tagged_sentence:\n",
    "        # Extract only chunks having NE labels.\n",
    "        if hasattr(tagged_tree, 'label'):\n",
    "            entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) # Get NE name.\n",
    "            entity_type = tagged_tree.label() # Get NE category.\n",
    "            named_entities.append((entity_name, entity_type))\n",
    "# Get unique named entities.\n",
    "named_entities = list(set(named_entities))\n",
    "\n",
    "# Store named entities in a data frame.\n",
    "entity_frame = pd.DataFrame(named_entities, \n",
    "                            columns=['Entity Name', 'Entity Type'])\n",
    "entity_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: SKIP Standford NER Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propositional Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign symbols and propositions.\n",
    "symbol_P = 'P'\n",
    "symbol_Q = 'Q'\n",
    "proposition_P = 'He is hungry'\n",
    "proposition_Q = 'He will eat a sandwich'\n",
    "\n",
    "# Assign various truth values to the proposition.\n",
    "p_statuses = [False, False, True, True]\n",
    "q_statuses = [False, True, False, True]\n",
    "\n",
    "# Assign the various expressions combining the logical operators.\n",
    "conjunction = '(P & Q)'\n",
    "disjunction = '(P | Q)'\n",
    "implication = '(P -> Q)'\n",
    "equivalence = '(P <-> Q)'\n",
    "expressions = [conjunction, disjunction, implication, equivalence]\n",
    "\n",
    "# Evaluate each expression using propositional logic.\n",
    "results = []\n",
    "\n",
    "for status_p, status_q in zip(p_statuses, q_statuses):\n",
    "    dom = set([])\n",
    "    val = nltk.Valuation([(symbol_P, status_p), \n",
    "                          (symbol_Q, status_q)])\n",
    "    assignments = nltk.Assignment(dom)\n",
    "    model = nltk.Model(dom, val)\n",
    "    row = [status_p, status_q]\n",
    "    for expression in expressions:\n",
    "        # Evaluate each expression based on proposition truth values.\n",
    "        result = model.evaluate(expression, assignments)\n",
    "        row.append(result)\n",
    "    results.append(row)\n",
    "\n",
    "# Build the result table.\n",
    "columns = [symbol_P, symbol_Q, conjunction, \n",
    "           disjunction, \n",
    "           implication,\n",
    "           equivalence]\n",
    "\n",
    "result_frame = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Display results.\n",
    "\n",
    "print('P:', proposition_P)\n",
    "print('Q:', proposition_Q)\n",
    "print()\n",
    "print('Expression Outcomes:-')\n",
    "result_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from module.normalization import normalize_corpus\n",
    "from module.utils import build_feature_matrix, display_evaluation_metrics, display_confusion_matrix, display_classification_report\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('movie_reviews.csv')\n",
    "dataset = dataset.head(10_000)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1_000 # 35_000\n",
    "train_data = dataset[:n]\n",
    "test_data = dataset[n:n+n]\n",
    "\n",
    "train_reviews = np.array(train_data['review'])\n",
    "train_sentiments = np.array(train_data['sentiment'])\n",
    "test_reviews = np.array(test_data['review'])\n",
    "test_sentiments = np.array(test_data['sentiment'])\n",
    "\n",
    "# Prepare sample dataset for experiments.\n",
    "# sample_docs = [100, 5817, 7626, 7356, 1008, 7155, 3533, 13010]\n",
    "sample_docs = [100, 581, 762, 735, 100, 715, 353, 130]\n",
    "sample_data = [(test_reviews[index], test_sentiments[index])\n",
    "               for index in sample_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization.\n",
    "norm_train_reviews = normalize_corpus(train_reviews, lemmatize=True, only_text_chars=True)\n",
    "\n",
    "# Feature extraction.\n",
    "vectorizer, train_features = build_feature_matrix(documents=norm_train_reviews,\n",
    "                                                  feature_type='tfidf',\n",
    "                                                  ngram_range=(1, 1),\n",
    "                                                  min_df=0.0,\n",
    "                                                  max_df=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Build the model.\n",
    "svm = SGDClassifier(loss='hinge', max_iter=200)\n",
    "svm.fit(train_features, train_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize reviews.\n",
    "norm_test_reviews = normalize_corpus(test_reviews, lemmatize=True, only_text_chars=True)\n",
    "\n",
    "# Extract features.\n",
    "test_features = vectorizer.transform(norm_test_reviews)\n",
    "\n",
    "# Predict sentiment for sample docs from test data.\n",
    "for doc_index in sample_docs:\n",
    "    print('Review:-')\n",
    "    print(test_reviews[doc_index])\n",
    "    print('Actual labeled sentiment:', test_sentiments[doc_index])\n",
    "    doc_features = test_features[doc_index]\n",
    "    predicted_sentiment = svm.predict(doc_features)[0]\n",
    "    print('Predicted sentiment:', predicted_sentiment)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the sentiment for test dataset movie reviews.\n",
    "predicted_sentiments = svm.predict(test_features)\n",
    "\n",
    "# Evaluate model prediction performance.\n",
    "# Show performance metrics.\n",
    "display_evaluation_metrics(true_labels=test_sentiments,\n",
    "                           predicted_labels=predicted_sentiments,\n",
    "                           positive_class='positive')\n",
    "\n",
    "# Show confusion matrix.\n",
    "display_confusion_matrix(true_labels=test_sentiments,\n",
    "                         predicted_labels=predicted_sentiments,\n",
    "                         classes=['positive', 'negative'])\n",
    "\n",
    "# Show detailed per-class classification report.\n",
    "display_classification_report(true_labels=test_sentiments,\n",
    "                              predicted_labels=predicted_sentiments,\n",
    "                              classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised lexicon-based techniques\n",
    "\n",
    "- AFINN lexicon\n",
    "- Bing Liu's lexicon\n",
    "- MPQA subjectivity lexicon\n",
    "- SentiWordNet\n",
    "- VADER lexicon\n",
    "- Pattern lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFINN Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "afn = Afinn(emoticons=True)\n",
    "afn.score('I really hated the plot of this movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afn.score('I really hated the plot of this movie :(')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# Get synset for 'good'.\n",
    "good = list(swn.senti_synsets('good', 'n'))[0]\n",
    "\n",
    "# Print synset sentiment scores.\n",
    "print('Positive polarity score:', good.pos_score())\n",
    "print('Negative polarity score:', good.neg_score())\n",
    "print('Objective score:', good.obj_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.normalization import normalize_accented_characters, strip_html\n",
    "import html\n",
    "\n",
    "def safe_list(l, i=0):\n",
    "    return l[i] if len(l) > i else None\n",
    "\n",
    "def analyze_sentiment_sentiwordnet_lexicon(review, verbose=False):\n",
    "    # Pre-process text.\n",
    "    review = normalize_accented_characters(review)\n",
    "    review = html.unescape(review.decode('utf-8'))\n",
    "    review = strip_html(review)\n",
    "    \n",
    "    # Tokenize and POS tag text tokens.\n",
    "    text_tokens = nltk.word_tokenize(review)\n",
    "    tagged_text = nltk.pos_tag(text_tokens)\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "    \n",
    "    # Get word synsets based on POS tags.\n",
    "    # Get sentiment scores if synsets are found.\n",
    "    for word, tag in tagged_text:\n",
    "        ss_set = None\n",
    "        if 'NN' in tag and swn.senti_synsets(word, 'n'):\n",
    "            ss_set = safe_list(list(swn.senti_synsets(word, 'n')))\n",
    "        elif 'VB' in tag and swn.senti_synsets(word, 'v'):\n",
    "            ss_set = safe_list(list(swn.senti_synsets(word, 'v')))\n",
    "        elif 'JJ' in tag and swn.senti_synsets(word, 'a'):\n",
    "            ss_set = safe_list(list(swn.senti_synsets(word, 'a')))\n",
    "        elif 'RB' in tag and swn.senti_synsets(word, 'r'):\n",
    "            ss_set = safe_list(list(swn.senti_synsets(word, 'r')))\n",
    "        \n",
    "        if ss_set:\n",
    "            # If senti-synset is found.\n",
    "            # Add scores for all found synsets.\n",
    "            pos_score += ss_set.pos_score()\n",
    "            neg_score += ss_set.neg_score()\n",
    "            obj_score += ss_set.obj_score()\n",
    "            token_count += 1\n",
    "    \n",
    "    # Aggregate final scores.\n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score)/token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score > 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        \n",
    "        # To display results in a nice table.\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score, \n",
    "                                         norm_pos_score,\n",
    "                                         norm_neg_score,\n",
    "                                         norm_final_score]],\n",
    "                                       columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'],\n",
    "                                                                     ['Predicted Sentiment',\n",
    "                                                                      'Objectivity',\n",
    "                                                                      'Positive',\n",
    "                                                                      'Negative',\n",
    "                                                                      'Overall']],\n",
    "                                                            codes=[[0,0,0,0,0], [0,1,2,3,4]]))\n",
    "        print(sentiment_frame)\n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review, review_sentiment in sample_data:\n",
    "    print('Review:')\n",
    "    print(review)\n",
    "    print()\n",
    "    print('Labeled sentiment:', review_sentiment)\n",
    "    print()\n",
    "    final_sentiment = analyze_sentiment_sentiwordnet_lexicon(review, verbose=True)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiment for test movie reviews dataset.\n",
    "sentiwordnet_predictions = [analyze_sentiment_sentiwordnet_lexicon(review)\n",
    "                            for review in test_reviews]\n",
    "\n",
    "# Get model performance statistics.\n",
    "print('Performance metrics:')\n",
    "display_evaluation_metrics(true_labels=test_sentiments,\n",
    "                           predicted_labels=sentiwordnet_predictions,\n",
    "                           positive_class='positive')\n",
    "print()\n",
    "print('Confusion Matrix:')\n",
    "display_confusion_matrix(true_labels=test_sentiments,\n",
    "                         predicted_labels=sentiwordnet_predictions,\n",
    "                         classes=['positive', 'negative'])\n",
    "\n",
    "print()\n",
    "print('Classification Report:')\n",
    "display_classification_report(true_labels=test_sentiments,\n",
    "                              predicted_labels=sentiwordnet_predictions,\n",
    "                              classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADER Lexicon\n",
    "\n",
    "VADER stands for Valence Aware Dictionary and sEntiment Reasoner. It is a lexicon with a rule-based sentiment analysis framework that was specially built for analyzing sentiment from social media resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_sentiment_vader_lexicon(review,\n",
    "                                    threhold=0.1,\n",
    "                                    verbose=False):\n",
    "    # Pre-process text.\n",
    "    review = normalize_accented_characters(review)\n",
    "    review = html.unescape(review.decode('utf-8'))\n",
    "    review = strip_html(review)\n",
    "    \n",
    "    # Analyze the sentiment for review.\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    \n",
    "    # Get aggregate scores and final sentiment.\n",
    "    agg_score = scores['compound']\n",
    "    \n",
    "    final_sentiment = 'positive' if agg_score >= threshold else 'negative'\n",
    "    \n",
    "    if verbose:\n",
    "        # Display detailed sentiment statistics.\n",
    "        positive = str(round(scores['pos'], 2) * 100) + '%'\n",
    "        final = round(agg_score, 2)\n",
    "        negative = str(round(scores['neg'], 2) * 100) + '%'\n",
    "        neutral = str(round(scores['neu'], 2) * 100) + '%'\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, final, positive, negative, neutral]],\n",
    "                                       columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'],\n",
    "                                                                     ['Predicted Sentiment',\n",
    "                                                                      'Polarity Score',\n",
    "                                                                      'Positive',\n",
    "                                                                      'Negative',\n",
    "                                                                      'Neutral']],\n",
    "                                                            labels=[[0, 0, 0, 0, 0], [0, 1, 2, 3, 4]]))\n",
    "        print(sentiment_frame)\n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed sentiment statistics.\n",
    "for review, review_sentiment in sample_data:\n",
    "    print('Review:')\n",
    "    print(review)\n",
    "    print()\n",
    "    print('Labeled Sentiment:', review_sentiment)\n",
    "    print()\n",
    "    final_sentiment = analyze_sentiment_vader_lexicon(review,\n",
    "                                                      threshold=0.1,\n",
    "                                                      verbose=True)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_predictions = [analyze_sentiment_vader_lexicon(review, threshold=0.1)\n",
    "                     for review in test_reviews]\n",
    "\n",
    "# Get model performance statistics.\n",
    "print('Performance metrics:')\n",
    "display_evaluation_metrics(true_labels=test_sentiments,\n",
    "                           predicted_labels=vader_predictions,\n",
    "                           positive_class='positive')\n",
    "\n",
    "print('\\nConfusion matrix:')\n",
    "display_confusion_matrix(true_labels=test_sentiments,\n",
    "                         predicted_labels=vader_predictions,\n",
    "                         classes=['positive', 'negative'])\n",
    "\n",
    "print('\\Classification report:')\n",
    "display_classfication_report(true_labels=test_sentiments,\n",
    "                             predicted_labels=vader_predictions,\n",
    "                             classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import sentiment, mood, modality\n",
    "\n",
    "def analyze_sentiment_pattern_lexicon(review, threshold=0.1, verbose=False):\n",
    "    # Pre-process text.\n",
    "    review = normalize_accented_characters(review)\n",
    "    review = html.unescape(review.decode('utf-8'))\n",
    "    review = strip_html(review)\n",
    "    \n",
    "    # Analyze sentiment for the text document.\n",
    "    analysis = sentiment(review)\n",
    "    sentiment_score = round(analysis[0], 2)\n",
    "    sentiment_subjectivity = round(analysis[1], 2)\n",
    "    \n",
    "    # Get final sentiment.\n",
    "    final_sentiment = 'positive' if sentiment_score >= threshold else 'negative'\n",
    "    if verbose:\n",
    "        # Display detailed sentiment statistics.\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, sentiment_score, sentiment_subjectivity]],\n",
    "                                       columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'],\n",
    "                                                                     ['Predicted Sentiment',\n",
    "                                                                      'Polarity Score',\n",
    "                                                                      'Subjectivity Score']],\n",
    "                                                             labels=[[0, 0, 0], [0, 1, 2]]))\n",
    "        print(sentiment_frame)\n",
    "        assessment = analysis.assessments\n",
    "        assessment_frame = pd.DataFrame(assessment,\n",
    "                                        columns=pd.MultiIndex(levels=[['DETAILED ASSESSMENT STATS:'],\n",
    "                                                                      ['Key Terms', 'Polarity Score',\n",
    "                                                                       'Subjectivity Score', 'Type']],\n",
    "                                                              labels=[[0, 0, 0, 0], [0,1,2,3]]))\n",
    "        print(assessment_frame)\n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed sentiment statistics.\n",
    "for review, review_sentiment in sample_data:\n",
    "    print('Review:')\n",
    "    print(review)\n",
    "    print()\n",
    "    print('Labeled sentiment:', review_sentiment)\n",
    "    print()\n",
    "    final_sentiment = analyze_sentiment_pattern_lexicon(review, threshold=0.1, verbose=True)\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review, review_sentiment in sample_data:\n",
    "    print('Review:')\n",
    "    print(review)\n",
    "    print('Labeled sentiment:', review_sentiment)\n",
    "    print('Mood:', mood(review))\n",
    "    mod_score = modality(review)\n",
    "    print('Modality score:', round(mod_score, 2))\n",
    "    print('Certainty: ', 'Strong' if mod_score > 0.5 else 'Medium' if mod_score > 0.35 else 'Low')\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiment for test movie reviews dataset.\n",
    "pattern_predictions = [analyze_sentiment_pattern_lexicon(review, threshold=0.1) for review in test_reviews]\n",
    "\n",
    "# Get model performance statistics.\n",
    "print('Performance statistics:')\n",
    "display_evaluation_metrics(true_labels=test_sentiments,\n",
    "                           predicted_labels=pattern_predictions,\n",
    "                           positive_class='positive')\n",
    "\n",
    "print('\\nConfusion matrix:')\n",
    "display_confusion_matrix(true_labels=test_sentiments,\n",
    "                         predicted_labels=pattern_predictions,\n",
    "                         classes=['positive', 'negative'])\n",
    "\n",
    "print('\\nClassification report:')\n",
    "display_classification_report(true_labels=test_sentiments,\n",
    "                              predicted_labels=pattern_predictions,\n",
    "                              classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
