{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification/categorization\n",
    "\n",
    "    What is text classification?\n",
    "\n",
    "Text classification is the process of assigning text documents into one or more classes or categories, assuming that we have a predefined set of classes.\n",
    "\n",
    "Documents here are textual documents, and each document can contain a sentence or even a paragraph of words. \n",
    "\n",
    "## Two types of text classification\n",
    "\n",
    "    What types of text classifications are available?\n",
    "\n",
    "- content-based classification\n",
    "- request-based classification\n",
    "\n",
    "__Content-based classification__ is the type of text classification where priorities or weights are given to a specific subjects or topics in the text content that would help determine the class of the document.\n",
    "\n",
    "E.g., a book with more than 30 percent of its content about food preparations can be classified under cooking/recipes. \n",
    "\n",
    "__Request-based classification__ is influenced by user requests and targeted towards specific user groups and audiences. This type of classification is governed by specific policies and ideals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification blueprint\n",
    "\n",
    "1. prepare test, train and validation (optional) datasets\n",
    "2. text normalization\n",
    "3. feature extraction\n",
    "4. model training\n",
    "5. model prediction and evaluation\n",
    "6. model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text normalization\n",
    "\n",
    "- expanding contractions\n",
    "- text standardization through lemmatization\n",
    "- removing special characters and aymbols\n",
    "- removing stopwords\n",
    "\n",
    "Others:\n",
    "- correcting spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use modules, create a directory module and a __init__.py file there.\n",
    "# Note that a .py file cannot be in the same folder as the .ipynb, else it will throw an exception.\n",
    "from module.contractions import expand_contractions \n",
    "from module.tokenize import tokenize_text\n",
    "from module.lemmatize import lemmatize_text, pos_tag_text\n",
    "from module.normalization import normalize_corpus\n",
    "from module.feature_extractors import bow_extractor, tfidf_transformer, tfidf_extractor, averaged_word_vectorizer, tfidf_weighted_averaged_word_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is not good'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions(\"this isn't good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define function to tokenize text into tokens that will be used by our other normalization functions.\n",
    "tokenize_text('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[hello] world'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Match any hello.\n",
    "pattern = re.compile('hello')\n",
    "\n",
    "# Define a substitution function that allows us access to the matched word.\n",
    "def subfn(m):\n",
    "    match = m.group(0)\n",
    "    return f'[{match}]'\n",
    "    \n",
    "pattern.sub(subfn, 'hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize_text('where are you playing football')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = [\n",
    "    'the sky is blue',\n",
    "    'sky is blue and sky is beautiful',\n",
    "    'the beautiful sky is blue',\n",
    "    'i love blue cheese'\n",
    "]\n",
    "new_doc = ['loving this blue sky today']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sky blue',\n",
       " ['sky', 'blue'],\n",
       " 'sky blue sky beautiful',\n",
       " ['sky', 'blue', 'sky', 'beautiful'],\n",
       " 'beautiful sky blue',\n",
       " ['beautiful', 'sky', 'blue'],\n",
       " 'love blue cheese',\n",
       " ['love', 'blue', 'cheese']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_corpus(CORPUS, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "\n",
    "### What is feature extraction/engineering?\n",
    "    \n",
    "- The process of extracting and selecting features\n",
    "\n",
    "### What is feature?\n",
    "\n",
    "- features are unique, measurable attributes or properties for each observation or data point in a dataset.\n",
    "- features are usuallu numeric in nature and can be absolute numeric values or categorical features that can be encoded as binary features for each category in the list using a process called __one-hot encoding__.\n",
    "\n",
    "### What are examples of feature extraction techniques?\n",
    "\n",
    "- bag of words model\n",
    "- tf-idf model\n",
    "- advanced word vectorization model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Bag of Words\n",
    "\n",
    "Disadvantage:\n",
    "- vectors are completely based on the absolute frequencies of word occurences\n",
    "- this may have potential problems where words that may tend to occur a lot across all documents in the corpus will have higher frequencies and will tend to overshadow other words that may not occur as frequently but may be more interesting and effective as features to identify specific categories for the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1, 0, 1, 0, 1, 1],\n",
       "        [1, 1, 1, 0, 2, 0, 2, 0],\n",
       "        [0, 1, 1, 0, 1, 0, 1, 1],\n",
       "        [0, 0, 1, 1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build bow vectorizer and get features.\n",
    "bow_vectorizer, bow_features = bow_extractor(CORPUS)\n",
    "features = bow_features.todense()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract features from new document using built vectorizer.\n",
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "new_doc_features = new_doc_features.todense()\n",
    "new_doc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'beautiful', 'blue', 'cheese', 'is', 'love', 'sky', 'the']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the feature names.\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def display_features(features, feature_names):\n",
    "    df = pd.DataFrame(data=features,\n",
    "                      columns=feature_names)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  the\n",
      "0    0          0     1       0   1     0    1    1\n",
      "1    1          1     1       0   2     0    2    0\n",
      "2    0          1     1       0   1     0    1    1\n",
      "3    0          0     1       1   0     1    0    0\n"
     ]
    }
   ],
   "source": [
    "display_features(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  the\n",
      "0    0          0     1       0   0     0    1    0\n"
     ]
    }
   ],
   "source": [
    "display_features(new_doc_features, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: TF-IDF Model\n",
    "\n",
    "- product of two metrics, term frequency (tf) and inverse document frequency (idf)\n",
    "- term frequency is the raw frequency value of that term in a particular document\n",
    "- $tf(w, D) = f_\\text(wD')$, $f_\\text(wD')$ denotes frequency for word in document D\n",
    "- inverse document frequency is the inverse of the document frequency for each term.\n",
    "- idf is computed by dividing the total number of documents in our corpus by the document frequency for each term and then applying logarithmic scaling on the result\n",
    "\n",
    "We add 1 to the document frequency for each term to indicate that we have one more document in our corpus that essentially has every term in the vocabulary. This is to prevent potential division-by-zero errors and smoothen the inverse document frequencies. We also add 1 to our result of our idf to avoid ignoring terms completely that might have zero idf:\n",
    "\n",
    "$idf(t) = 1 + log\\frac{C}{1 + df(t)}$\n",
    "\n",
    "Where:\n",
    "- $C$ is the count of the total number of documents in our corpus\n",
    "- $idf(t)$ is the idf for term t\n",
    "- $df(t)$ is the frequency of the number of documents in which term t is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00\n",
      "2  0.00       0.52  0.34    0.00  0.42  0.00  0.42  0.52\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Build tfidf transformer and show train corpus tfidf features.\n",
    "tfidf_trans, tfidf_features = tfidf_transformer(bow_features)\n",
    "features = np.round(tfidf_features.todense(), 2)\n",
    "display_features(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0\n"
     ]
    }
   ],
   "source": [
    "# Show tfidf features for new_doc using built tfidf transformer.\n",
    "nd_tfidf = tfidf_trans.transform(new_doc_features)\n",
    "nd_features = np.round(nd_tfidf.todense(), 2)\n",
    "display_features(nd_features, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing TF-IDF from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "\n",
    "# Compute term frequency.\n",
    "tf = bow_features.todense()\n",
    "tf = np.array(tf, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love  sky  the\n",
      "0  0.0        0.0   1.0     0.0  1.0   0.0  1.0  1.0\n",
      "1  1.0        1.0   1.0     0.0  2.0   0.0  2.0  0.0\n",
      "2  0.0        1.0   1.0     0.0  1.0   0.0  1.0  1.0\n",
      "3  0.0        0.0   1.0     1.0  0.0   1.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# Show term frequency.\n",
    "display_features(tf, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the document frequency matrix.\n",
    "df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n",
    "df = 1 + df # To smoothen the idf later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  the\n",
      "0    2          3     5       2   4     2    4    3\n"
     ]
    }
   ],
   "source": [
    "# How many times the term appear in each document + 1.\n",
    "display_features([df], feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute inverse document frequencies.\n",
    "total_docs = 1 + len(CORPUS)\n",
    "idf = 1.0 + np.log(float(total_docs) / df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky   the\n",
      "0  1.92       1.51   1.0    1.92  1.22  1.92  1.22  1.51\n"
     ]
    }
   ],
   "source": [
    "# Show inverse document frequencies.\n",
    "display_features([np.round(idf, 2)], feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute idf diagonal matrix.\n",
    "total_features = bow_features.shape[1]\n",
    "idf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features)\n",
    "idf = idf_diag.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.92, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.51, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 1.92, 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 1.22, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 1.92, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.22, 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.51]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(idf, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky   the\n",
      "0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  1.51\n",
      "1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00\n",
      "2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.51\n",
      "3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "tfidf = tf * idf\n",
    "display_features(np.round(tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute L2 norms.\n",
    "norms = norm(tfidf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.5 , 4.35, 2.93, 2.89])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print norms for each document.\n",
    "np.round(norms, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.39921021, 0.        , 0.48829139,\n",
       "         0.        , 0.48829139, 0.60313701],\n",
       "        [0.44051607, 0.34730793, 0.22987956, 0.        , 0.5623514 ,\n",
       "         0.        , 0.5623514 , 0.        ],\n",
       "        [0.        , 0.51646957, 0.34184591, 0.        , 0.41812662,\n",
       "         0.        , 0.41812662, 0.51646957],\n",
       "        [0.        , 0.        , 0.34618161, 0.66338461, 0.        ,\n",
       "         0.66338461, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute normalized tfidf.\n",
    "norm_tfidf = tfidf / norms[:, None]\n",
    "norm_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00\n",
      "2  0.00       0.52  0.34    0.00  0.42  0.00  0.42  0.52\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# Show final tfidf feature matrix.\n",
    "display_features(np.round(norm_tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute new doc terms freqs from bow freqs.\n",
    "nd_tf = new_doc_features\n",
    "nd_tf = np.array(nd_tf, dtype='float64')\n",
    "\n",
    "# Compute tfidf using idf matrix from train corpus.\n",
    "nd_tfidf = nd_tf * idf\n",
    "nd_norms = norm(nd_tfidf, axis=1)\n",
    "norm_nd_tfidf = nd_tfidf / nd_norms[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0\n"
     ]
    }
   ],
   "source": [
    "# Show new_doc tfidf feature vector.\n",
    "display_features(np.round(norm_nd_tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00\n",
      "2  0.00       0.52  0.34    0.00  0.42  0.00  0.42  0.52\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# Build tfidf vectorizer and get training corpus feature vectors.\n",
    "tfidf_vectorizer, tdidf_features = tfidf_extractor(CORPUS)\n",
    "display_features(np.round(tdidf_features.todense(), 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0\n"
     ]
    }
   ],
   "source": [
    "# Get tfidf feature vector for the new document.\n",
    "nd_tfidf = tfidf_vectorizer.transform(new_doc)\n",
    "display_features(np.round(nd_tfidf.todense(), 2), feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Word Vectorization Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = [\n",
    "    'the sky is blue',\n",
    "    'sky is blue and sky is beautiful',\n",
    "    'the beautiful sky is blue',\n",
    "    'i love blue cheese'\n",
    "]\n",
    "new_doc = ['loving this blue sky today']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize corpora.\n",
    "TOKENIZED_CORPUS = [nltk.word_tokenize(sentence)\n",
    "                    for sentence in CORPUS]\n",
    "tokenized_new_doc = [nltk.word_tokenize(sentence)\n",
    "                     for sentence in new_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the word2vec model on our training corpus.\n",
    "\n",
    "# size: set the size or dimension for the word vectors.\n",
    "# window: set the context or window size, which specifies the length of the window of words that should be considered for the algorithm to take into account as context when training.\n",
    "# min_count: the minimum word count needed across the corpus for the words to be considered in the vocabulary.\n",
    "# sample: used to downsample effects of occurence of frequent words.\n",
    "model = gensim.models.Word2Vec(TOKENIZED_CORPUS, size=10, window=10,\n",
    "                               min_count=2, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04561428, -0.02996336, -0.04556778, -0.00351243,  0.04590314,\n",
       "        0.01923104, -0.0222545 ,  0.01540163, -0.04887827, -0.03940738],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['sky']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00696618, -0.03747655,  0.00500463, -0.02279691,  0.0324855 ,\n",
       "       -0.04436615,  0.03656287,  0.03615776,  0.00809231,  0.01606599],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['blue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaged Word Vectors\n",
    "\n",
    "Problem:\n",
    "- each word vector is of length 10 based on the size parameter specified earlier.\n",
    "- but sentences are of unequal length\n",
    "- some operations (combining and aggregations) are required to make sure the number of dimensions of the final feature vectors are the same, regardless of the length of the text document, number of words and so on. \n",
    "\n",
    "\n",
    "Solution:\n",
    "- use average weighted word vectorization scheme, where for each text document we will extract all the tokens of the text document, and for each token in the document we will capture the subsequent word vector if present in the vocabulary. \n",
    "- we will sum up all the word vectors and divide the result by the total number of words matched in the vocabulary to get a final resulting averaged word vector representation of the text document.\n",
    "\n",
    "Pseudo-code:\n",
    "```\n",
    "model := the word2vec model we built\n",
    "vocabulary := unique_words(model)\n",
    "document := [words]\n",
    "matched_word_count := 0\n",
    "vector := []\n",
    "\n",
    "for word in words:\n",
    "    if word in vocabulary:\n",
    "        vector := vector + model[word]\n",
    "        matched_word_count := matched_word_count + 1\n",
    "\n",
    "averaged_word_vector := vector / matched_word_count\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.006, -0.014, -0.001, -0.008,  0.017, -0.008, -0.013,  0.027,\n",
       "        -0.015, -0.015],\n",
       "       [ 0.01 , -0.025, -0.013, -0.005,  0.014, -0.003, -0.007,  0.019,\n",
       "        -0.014, -0.021],\n",
       "       [ 0.005, -0.011,  0.005, -0.008,  0.006,  0.   , -0.003,  0.024,\n",
       "        -0.017, -0.01 ],\n",
       "       [-0.007, -0.037,  0.005, -0.023,  0.032, -0.044,  0.037,  0.036,\n",
       "         0.008,  0.016]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ge averaged word vectors for our training CORPUS.\n",
    "avg_word_vec_features = averaged_word_vectorizer(corpus=TOKENIZED_CORPUS,\n",
    "                                                 model=model,\n",
    "                                                 num_features=10)\n",
    "np.round(avg_word_vec_features, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.019, -0.034, -0.02 , -0.013,  0.039, -0.013,  0.007,  0.026,\n",
       "        -0.02 , -0.012]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get averaged word vectors for our test new_doc.\n",
    "nd_avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_new_doc, \n",
    "                                                    model=model,\n",
    "                                                    num_features=10)\n",
    "np.round(nd_avg_word_vec_features, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Weighted Average Word Vectors\n",
    "\n",
    "```\n",
    "model := the word2vec model we built\n",
    "vocabulary := unique_words(model)\n",
    "document := [words]\n",
    "tfidfs := [tfidf(word) for each word in words]\n",
    "matched_word_wts := 0\n",
    "vector := []\n",
    "\n",
    "for word in words:\n",
    "    if word in vocabulary:\n",
    "        word_vector := model[word]\n",
    "        weighted_word_vector := tfidfs[word] x word_vector\n",
    "        vector := vector + weighted_word_vector\n",
    "        matched_word_wts := matched_word_wts + tfidfs[word]\n",
    "\n",
    "tfidf_wtd_avgd_word_vector := vector / matched_word_wts\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.007, -0.01 ,  0.002, -0.008,  0.015, -0.005, -0.016,  0.027,\n",
       "        -0.017, -0.016],\n",
       "       [ 0.012, -0.025, -0.018, -0.002,  0.016, -0.   , -0.016,  0.017,\n",
       "        -0.015, -0.028],\n",
       "       [ 0.005, -0.008,  0.007, -0.008,  0.003,  0.004, -0.003,  0.024,\n",
       "        -0.019, -0.01 ],\n",
       "       [-0.007, -0.037,  0.005, -0.023,  0.032, -0.044,  0.037,  0.036,\n",
       "         0.008,  0.016]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get tfidf weights and vocabulary from earlier results and compute result.\n",
    "corpus_tfidf = tdidf_features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=TOKENIZED_CORPUS, \n",
    "                                                                     tfidf_vectors=corpus_tfidf,\n",
    "                                                                     tfidf_vocabulary=vocab,\n",
    "                                                                     model=model,\n",
    "                                                                     num_features=10)\n",
    "np.round(wt_tfidf_word_vec_features, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute avgd word vector for test new_doc.\n",
    "nd_wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_new_doc,\n",
    "                                                                        tfidf_vectors=nd_tfidf,\n",
    "                                                                        tfidf_vocabulary=vocab,\n",
    "                                                                        model=model,\n",
    "                                                                        num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.022, -0.033, -0.023, -0.012,  0.04 , -0.009,  0.004,  0.025,\n",
       "        -0.023, -0.014]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(nd_wt_tfidf_word_vec_features, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
